Graylog : Visualisation et Analyse

Graylog est une plateforme centralisée de gestion et d’analyse des logs. Elle permet de collecter, traiter, et visualiser des journaux provenant de diverses sources pour une surveillance et une analyse efficaces.
	•	Fonctionnalités principales :
	•	Tableaux de bord (Dashboards) : Permettent de visualiser les métriques clés et les journaux critiques en temps réel.
	•	Recherches avancées : Recherche puissante basée sur des filtres pour analyser rapidement les événements.
	•	Alertes : Notifications configurables pour détecter et réagir rapidement à des conditions spécifiques.
	•	Pipelines de traitement : Manipulation des données brutes avant stockage ou visualisation.
	•	Architecture :
	•	Graylog se connecte à OpenSearch pour le stockage des données.
	•	Les données sont reçues via divers inputs (comme GELF Kafka ou Syslog).
	•	Avantages :
	•	Interface utilisateur conviviale pour les administrateurs.
	•	Extensibilité via des plugins pour répondre à des besoins spécifiques.
	•	Sécurisation des données grâce au chiffrement.




OpenSearch : Stockage des Logs et Recherche

OpenSearch est un moteur d’analyse et de recherche distribué. Il est utilisé par Graylog pour indexer et stocker les logs.
	•	Fonctionnalités principales :
	•	Indexation rapide : Permet de gérer de grands volumes de données en temps réel.
	•	Recherche puissante : Basée sur des requêtes avancées (syntax JSON) pour explorer les logs efficacement.
	•	Visualisation native : Avec OpenSearch Dashboards pour une analyse complémentaire.
	•	Configuration pour Graylog :
	•	Templates d’index : Configurés dans Graylog pour structurer les données dans OpenSearch.
	•	Rétention des données : Les logs peuvent être archivés ou supprimés après une durée prédéfinie pour économiser de l’espace.
	•	Avantages :
	•	Haute scalabilité pour gérer de grands ensembles de données.
	•	Open-source et activement maintenu.
	•	Intégration fluide avec Graylog.


MongoDB : Métadonnées et Gestion de la Configuration

MongoDB est une base de données NoSQL utilisée par Graylog pour stocker des métadonnées et des configurations.
	•	Rôles dans l’architecture Graylog :
	•	Configurations des Inputs : MongoDB stocke les informations sur les inputs Graylog (ex. GELF Kafka, Syslog).
	•	Utilisateurs et Permissions : Gestion des comptes utilisateurs et des rôles.
	•	Configurations des Dashboards : Stocke la disposition et les widgets des tableaux de bord créés dans Graylog.
	•	Pipelines et Streams : Conserve les règles et configurations pour traiter les journaux.
	•	Avantages de MongoDB :
	•	Structure flexible, idéale pour stocker des configurations variées.
	•	Rapide et évolutif, adapté aux systèmes avec un volume croissant de métadonnées.
	•	Sauvegardes faciles pour préserver les paramètres critiques.
	•	Bonnes pratiques :
	•	Sauvegardes régulières : Assurez-vous que MongoDB est sauvegardé pour éviter la perte de configurations critiques.
	•	Monitoring : Surveillez MongoDB pour détecter les ralentissements ou les erreurs de connexion.




Kafka : Transport et Gestion des Logs

Kafka est une plateforme de messagerie distribuée conçue pour gérer des flux de données en temps réel avec une haute fiabilité et scalabilité. Dans un SIEM, Kafka sert de pipeline centralisé pour transporter les journaux depuis les agents collecteurs vers les consommateurs comme Logstash ou Graylog.
	•	Fonctionnalités principales :
	•	Topics : Kafka organise les données en topics, chaque topic contenant des messages pertinents pour une source ou un flux spécifique.
	•	Partitions : Les topics sont divisés en partitions pour permettre un traitement parallèle et augmenter la scalabilité.
	•	Rétention des messages : Kafka conserve les messages pendant une durée ou une taille configurée, permettant aux consommateurs de relire les messages en cas de besoin.
	•	Fiabilité : Grâce à des mécanismes de réplication et d’acknowledgements, Kafka garantit que les messages sont correctement livrés.
	•	Configuration pour le SIEM :
	•	Producers :
	•	Filebeat, Winlogbeat et Rsyslog envoient les logs vers Kafka en tant que producteurs.
	•	Consumers :
	•	Logstash consomme les logs pour transformation et enrichissement.
	•	Sécurisation :
	•	Configurez TLS/SSL pour sécuriser les communications entre producteurs, consommateurs et brokers Kafka.
	•	Gestion des limites :
	•	Rétention des logs : Par exemple, log.retention.hours pour conserver les données 4 jours.
	•	Taille maximale : Par exemple, log.retention.bytes pour limiter à 450 Go par topic.
	•	Avantages :
	•	Tolérance aux pannes grâce à la réplication.
	•	Haute performance pour gérer des millions de messages par seconde.
	•	Flexibilité pour connecter divers producteurs et consommateurs.




Kafka Streams : Duplication et Traitement des Topics Kafka

Kafka Streams est une API de traitement en flux fournie par Kafka, utilisée pour traiter, transformer ou dupliquer les données en transit dans Kafka.
	•	Rôles dans l’architecture SIEM :
	•	Duplication des topics :
	•	Kafka Streams peut dupliquer les messages d’un topic source vers plusieurs topics de destination.
	•	Exemples : raw-logs dupliqué en processed-logs et archived-logs.
	•	Enrichissement des logs :
	•	Kafka Streams peut transformer ou enrichir les messages avant qu’ils n’atteignent les consommateurs.
	•	Partitionnement :
	•	Kafka Streams peut re-partitionner les données pour améliorer la parallélisation des consommateurs.
	•	Configuration typique :
	•	Application Kafka Streams déployée avec des propriétés comme :
	•	application.id : Identifiant unique de l’application Kafka Streams.
	•	bootstrap.servers : Liste des serveurs Kafka.
	•	Sécurisation : TLS/SSL pour les communications.
	•	Utilisation de l’API pour lire un topic source, effectuer des transformations ou une duplication, et écrire dans les topics de destination.
	•	Avantages :
	•	Traitement des flux en temps réel.
	•	Déploiement facile en tant que service indépendant.
	•	Compatible avec les mécanismes de réplication et de rétention de Kafka.

Logstash : Transformation des Logs

Logstash est un pipeline de traitement des données qui collecte, enrichit, et achemine les logs vers diverses destinations, comme Kafka ou Graylog.
	•	Rôles dans l’architecture SIEM :
	•	Entrées (inputs) :
	•	Kafka : Consomme les logs bruts des topics Kafka pour transformation.
	•	Filtres :
	•	Transformation des données : Parsing JSON, extraction de champs, normalisation.
	•	Enrichissement : Ajout de métadonnées comme geoip ou tags.
	•	Suppression de champs inutiles ou masquage des données sensibles.
	•	Sorties (outputs) :
	•	Kafka : Envoi des données enrichies dans un nouveau topic.
	•	Graylog (via GELF) : Transmission directe des logs transformés pour visualisation.
	•	Exemple de Pipeline Typique :




input {
  kafka {
    bootstrap_servers => "kafka-1:9092"
    topics => ["raw-logs"]
    codec => "json"
  }
}

filter {
  # Parsing JSON
  json {
    source => "message"
  }

  # Ajouter des métadonnées
  mutate {
    add_field => { "processed" => "true" }
  }

  # Supprimer les champs inutiles
  mutate {
    remove_field => ["[headers]", "[raw]"]
  }
}

output {
  kafka {
    bootstrap_servers => "kafka-1:9092"
    topic_id => "processed-logs"
  }
}



	•	Avantages :
	•	Extrêmement flexible avec ses nombreux plugins.
	•	Supporte un large éventail de sources et de destinations.
	•	Gestion avancée des erreurs et tolérance aux pannes avec des queues persistantes.



Agents Beats

Filebeat : Collecte des Logs de Fichiers

Filebeat est un agent léger utilisé pour collecter des logs de fichiers locaux et les envoyer vers des destinations comme Kafka, Logstash, ou Elasticsearch.
	•	Fonctionnalités principales :
	•	Collecte des logs : Suit les fichiers en temps réel et détecte automatiquement les nouvelles lignes.
	•	Modules prédéfinis : Inclut des configurations spécifiques pour collecter les logs d’applications comme NGINX, Apache, et MySQL.
	•	Gestion des fichiers rotatifs : Continue à lire les fichiers même lorsqu’ils sont renommés ou archivés.
	•	Tolérance aux pannes : Stocke un état persistant pour éviter de perdre des logs en cas de redémarrage.
	•	Configuration Typique :

filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/messages
      - /var/log/syslog

output.kafka:
  hosts: ["kafka-1:9092", "kafka-2:9092"]
  topic: "raw-logs"
  ssl.enabled: true
  ssl.certificate_authorities: ["/etc/filebeat/ssl/ca.crt"]



	Avantages :
	•	Faible consommation de ressources.
	•	Intégration simple avec les autres outils Elastic Stack ou Kafka.
	•	Extensible via des modules.

Winlogbeat : Collecte des Logs Windows

Winlogbeat est un agent spécifique pour collecter les journaux d’événements Windows.
	•	Fonctionnalités principales :
	•	Collecte les logs des journaux d’événements Windows (Application, Système, Sécurité).
	•	Permet des filtres avancés pour choisir les événements spécifiques à collecter.
	•	Envoi direct vers Kafka, Logstash ou Elasticsearch.
	•	Configuration Typique :


winlogbeat.event_logs:
  - name: Application
  - name: Security
    level: critical, error

output.kafka:
  hosts: ["kafka-1:9092"]
  topic: "windows-logs"
  ssl.enabled: true
  ssl.certificate_authorities: ["/etc/winlogbeat/ssl/ca.crt"]



Avantages :
	•	Idéal pour la surveillance des systèmes Windows.
	•	Léger et facile à configurer.
	•	Permet de surveiller les attaques ou incidents à partir des journaux de sécurité.

Graylog Sidecars

Graylog Sidecars est un composant utilisé pour gérer les agents de collecte (Filebeat, Winlogbeat, ou Rsyslog) depuis l’interface Graylog.
	•	Fonctionnalités principales :
	•	Simplifie la configuration des agents sur les serveurs.
	•	Gère les agents à distance via l’interface Graylog.
	•	Intègre un contrôle centralisé pour des centaines de serveurs.
	•	Configuration Typique :
	1.	Installez le Graylog Sidecar sur un serveur cible.
	2.	Associez un agent, comme Filebeat ou Winlogbeat.
	3.	Configurez le Sidecar dans Graylog :
	•	Créez une configuration Filebeat dans Graylog Web UI.
	•	Déployez-la sur les serveurs Sidecar.
	4.	Exemple de configuration pour Filebeat via Sidecar :



filebeat.inputs:
  - type: log
    paths:
      - /var/log/application.log
      - /var/log/error.log

output.kafka:
  hosts: ["kafka-1:9092"]
  topic: "application-logs"



	Avantages :
	•	Administration centralisée des agents.
	•	Gain de temps pour déployer ou modifier les configurations.
	•	Permet de gérer des environnements complexes.

Rsyslog : Collecte et Transmission des Logs

Rsyslog est un démon de journalisation traditionnel pour Linux, utilisé pour collecter et transférer des logs système vers des destinations locales ou distantes.
	•	Fonctionnalités principales :
	•	Collecte des logs locaux : Capture les journaux système (syslog, auth.log, etc.).
	•	Transmission distante : Peut envoyer les logs via TCP/UDP à des serveurs distants.
	•	Support des protocoles modernes : Compatible avec RELP pour des transmissions fiables.
	•	Filtrage avancé : Permet de trier et rediriger les logs en fonction de critères comme le programme, l’adresse IP ou la sévérité.
	•	Configuration Typique :
	•	Envoi vers Kafka :




module(load="omkafka")
action(type="omkafka" topic="syslog-topic"
  broker=["kafka-1:9092"]
  template="RSYSLOG_ForwardFormat")




	Avantages :
	•	Très performant pour la gestion des logs système.
	•	Léger et extensible avec des modules.
	•	Fiabilité accrue avec le module RELP.


Comparaison Agents Beats, Sidecars et Rsyslog


Composant	Collecte	Envoi	Cas d’utilisation
Filebeat	Fichiers	Kafka, Logstash	Logs d’applications (Linux, Windows).
Winlogbeat	Windows Event Logs	Kafka, Logstash	Journaux Windows pour les systèmes.
Sidecar	Interface	Gère Beats et Rsyslog	Centralisation des configurations.
Rsyslog	Journaux Système	Kafka, Syslog	Logs système Unix/Linux, forwarding

Ces composants sont essentiels pour une architecture SIEM robuste, couvrant les systèmes Linux, Windows, et des besoins spécifiques d’analyse des logs.


