filter {
  ruby {
    code => '
      # Récupère la valeur du champ [message][group]
      group_value = event.get("[message][group]")
      if group_value
        # Remplace tous les backslashes par rien
        group_value = group_value.gsub(/\\/, "")
        # Met à jour le champ avec la valeur nettoyée
        event.set("[message][group]", group_value)
      end
    '
  }
}

# Load necessary modules
module(load="imtcp")          # Input TCP module (if receiving logs via TCP)
module(load="mmjsonparse")    # JSON parsing module
module(load="omfile")         # File output module

# Define a TCP input for logs if required
input(type="imtcp" port="514")

# Parse messages only if they come from host 192.167.1.1
if ($fromhost-ip == "192.167.1.1") then {
    # Parse the JSON message
    *.* :mmjsonparse:

    # Define a template to clean the 'group' field
    template(name="CleanedJson" type="string" string="
    {
      \"log_id\": \"%$.log_id%\",
      \"type\": \"%$.type%\",
      \"subtype\": \"%$.subtype%\",
      \"level\": \"%$.level%\",
      \"vd\": \"%$.vd%\",
      \"srcip\": \"%$.srcip%\",
      \"srcport\": \"%$.srcport%\",
      \"srcintf\": \"%$.srcintf%\",
      \"dstip\": \"%$.dstip%\",
      \"dstport\": \"%$.dstport%\",
      \"dstintf\": \"%$.dstintf%\",
      \"protocol\": \"%$.protocol%\",
      \"action\": \"%$.action%\",
      \"policyid\": \"%$.policyid%\",
      \"service\": \"%$.service%\",
      \"duration\": \"%$.duration%\",
      \"sentbyte\": \"%$.sentbyte%\",
      \"group\": \"%property_replace($.group, '\\\\', '')%\",
      \"rcvdbyte\": \"%$.rcvdbyte%\",
      \"direction\": \"%$.direction%\",
      \"logdesc\": \"%$.logdesc%\",
      \"appcat\": \"%$.appcat%\",
      \"eventtime\": \"%$.eventtime%\",
      \"msg\": \"%$.msg%\",
      \"devname\": \"%$.devname%\",
      \"devid\": \"%$.devid%\",
      \"timestamp\": \"%$.timestamp%\",
      \"status\": \"%$.status%\",
      \"hostname\": \"%$.hostname%\"
    }
    ")

    # Write the cleaned JSON message to a specific file
    action(type="omfile"
           file="/var/log/cleaned_json_19216711.log"
           template="CleanedJson")
}


# Load modules for JSON parsing and Kafka output
module(load="omkafka")
module(load="mmjsonparse")  # To parse JSON messages

# Parse the JSON input message
*.* :mmjsonparse:

# Create a template to clean up a specific JSON field
template(name="CleanedJsonKafkaMessage" type="string" string="
{
  \"field1\": \"%$.field1%\",
  \"field2\": \"%property_replace($.field2, '\\', '')%\",
  \"field3\": \"%$.field3%\"
}
")

# Kafka output action using the cleaned template
action(
    type="omkafka"
    broker="kafka-broker:9092"   # Replace with your Kafka broker details
    topic="cleaned_logs"         # Replace with your Kafka topic
    template="CleanedJsonKafkaMessage"  # Use the template with cleaned JSON field
)

# Load necessary modules
module(load="omkafka")  # Kafka output module

# Define a template to clean the backslash
template(name="CleanedKafkaMessage" type="string" string="%property_replace(msg, '\\', '')%")

# Configure omkafka action
action(
    type="omkafka"
    broker="kafka-broker:9092"  # Replace with your Kafka broker details
    topic="cleaned_logs"        # Replace with your Kafka topic
    template="CleanedKafkaMessage"  # Use the template to remove backslashes
)


module(load="imuxsock")
module(load="imklog")
module(load="omkafka")  # Module pour envoyer les messages vers Kafka

# Définition d'un template simple en texte brut
template(name="PlainTemplate" type="list") {
    constant(value="syslog message ")
    property(name="msg")
    constant(value="\n")
}

# Condition si l'IP source est 192.168.1.10
if $fromhost-ip == "192.168.1.10" then {
    # Envoi vers un topic spécifique si l'IP correspond
    action(
        type="omkafka"
        broker="kafka1:9092,kafka2:9092"    # Liste des brokers Kafka
        topic="special_topic"               # Topic si IP = 192.168.1.10
        template="PlainTemplate"            # Utilisation du template texte brut
    )
} else {
    # Sinon, envoi vers un topic par défaut
    action(
        type="omkafka"
        broker="kafka1:9092,kafka2:9092"
        topic="default_topic"
        template="PlainTemplate"
    )
}



ki kafka - 
listeners=SSL://0.0.0.0:9093
ssl.keystore.location=/path/to/server.keystore.jks
ssl.keystore.password=keystore_password
ssl.truststore.location=/path/to/server.truststore.jks
ssl.truststore.password=truststore_password
ssl.client.auth=required

rsyslog : 
module(load="omkafka")

template(name="kafkaFormat" type="string" string="%msg%\n")

action(
  type="omkafka"
  topic="secure-logs"
  broker=["broker1:9093", "broker2:9093"]
  template="kafkaFormat"
  confParam=[
    "security.protocol=SSL",
    "ssl.truststore.location=/etc/rsyslog/certs/kafka.truststore.jks",
    "ssl.truststore.password=truststore_password",
    "ssl.keystore.location=/etc/rsyslog/certs/kafka.keystore.jks",
    "ssl.keystore.password=keystore_password",
    "ssl.key.password=key_password"
  ]
)

redpanda-console : 

kafka:
  brokers:
    - broker1:9093
    - broker2:9093
  tls:
    enabled: true
    caFile: /path/to/ca.crt
    certFile: /path/to/client.crt
    keyFile: /path/to/client.key

logstash : 

output {
  kafka {
    bootstrap_servers => "broker1:9093,broker2:9093"
    topic_id => "secure-logs"
    security_protocol => "SSL"
    ssl_truststore_location => "/path/to/truststore.jks"
    ssl_truststore_password => "truststore_password"
    ssl_keystore_location => "/path/to/keystore.jks"
    ssl_keystore_password => "keystore_password"
    ssl_key_password => "key_password"
  }
}

input GELF Kafka 
security.protocol=SSL
ssl.truststore.location=/etc/graylog/certs/truststore.jks
ssl.truststore.password=yourTruststorePassword
ssl.keystore.location=/etc/graylog/certs/keystore.jks
ssl.keystore.password=yourKeystorePassword
ssl.key.password=




import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.StreamsConfig;

import java.util.Properties;

public class DuplicateTopicExample {
    public static void main(String[] args) {
        // Configuration de Kafka Streams
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "duplicate-topic-example");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

        // Construction de la topologie
        StreamsBuilder builder = new StreamsBuilder();

        // Lire le topic source
        KStream<String, String> sourceStream = builder.stream("source-topic");

        // Écrire les messages vers deux topics différents
        sourceStream.to("target-topic-1");
        sourceStream.to("target-topic-2");

        // Démarrer l'application Kafka Streams
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();

        // Arrêter proprement Kafka Streams à l'arrêt du programme
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}


from kafka import KafkaConsumer, KafkaProducer

# Configuration du consommateur et du producteur
consumer = KafkaConsumer(
    'source-topic',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    group_id='duplication-group'
)
producer = KafkaProducer(bootstrap_servers=['localhost:9092'])

# Lecture et duplication des messages
for message in consumer:
    producer.send('target-topic-1', key=message.key, value=message.value)
    producer.send('target-topic-2', key=message.key, value=message.value)





