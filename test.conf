kafka-run-class.sh kafka.tools.ProducerPerformance \
  --topic my_topic \
  --num-records 100000 \
  --record-size 100 \
  --throughput -1 \
  --producer-props bootstrap.servers=kafka:9092

Le LAG Kafka reprÃ©sente le nombre de messages en attente de traitement par les consommateurs. Dans ton cas, aprÃ¨s lâ€™incident de stockage, le LAG est passÃ© de 8M Ã  38M, ce qui signifie que les consommateurs nâ€™ont pas pu traiter les messages en temps rÃ©el et ont pris du retard.

ðŸ“Œ Cause principale du problÃ¨me :
	1.	Stockage plein â†’ Kafka ne pouvait plus Ã©crire de nouveaux messages, ce qui a interrompu le traitement normal des consommateurs.
	2.	RedÃ©marrage du cluster â†’ Une fois lâ€™espace disque libÃ©rÃ©, les producteurs ont recommencÃ© Ã  envoyer des messages, mais les consommateurs avaient un Ã©norme retard Ã  rattraper.
	3.	GÃ©nÃ©ration rapide de nouveaux messages â†’ Pendant que les consommateurs essaient de rattraper leur retard, de nouveaux messages continuent dâ€™arriver, augmentant encore le LAG.
	4.	DÃ©bit des consommateurs insuffisant â†’ Si Logstash ne consomme pas assez vite, il ne peut pas rÃ©duire le LAG efficacement.

ðŸ“Œ Comment rÃ©soudre le problÃ¨me ?

1ï¸âƒ£ VÃ©rifier la capacitÃ© des consumers Logstash

Tes consumers peuvent Ãªtre limitÃ©s par plusieurs facteurs :
âœ… CPU & RAM : VÃ©rifie que Logstash nâ€™est pas CPU ou RAM-bound. Sâ€™il est en surcharge, il ne pourra pas traiter les messages rapidement.
âœ… Workers et Threads : VÃ©rifie la configuration de pipeline.workers et pipeline.batch.size dans Logstash (logstash.yml). Tu peux augmenter :

pipeline.workers: 4  # Augmente si la machine a plus de CPU
pipeline.batch.size: 200
pipeline.batch.delay: 50

âœ… Parallelisme Kafka Input : Si tu utilises logstash-input-kafka, assure-toi dâ€™avoir plusieurs threads (consumer_threads):

input {
  kafka {
    bootstrap_servers => "kafka1:9092,kafka2:9092,kafka3:9092"
    topics => ["mon_topic"]
    group_id => "logstash_group"
    consumer_threads => 4  # Augmente le nombre de threads
  }
}

2ï¸âƒ£ VÃ©rifier la charge du cluster Kafka

âœ… Nombre de partitions par topic :
	â€¢	Plus tu as de partitions, plus les consommateurs peuvent lire en parallÃ¨le.
	â€¢	VÃ©rifie avec la commande :

kafka-topics.sh --describe --topic mon_topic --bootstrap-server kafka:9092


	â€¢	Si ton topic nâ€™a quâ€™une ou deux partitions et plusieurs consommateurs, il peut y avoir une saturation. Dans ce cas, augmente le nombre de partitions :

kafka-topics.sh --alter --topic mon_topic --partitions 10 --bootstrap-server kafka:9092

âš ï¸ Attention : Ne change pas le nombre de partitions une fois un topic en production sans bien gÃ©rer le rebalancement !

âœ… Charge des Brokers Kafka :
	â€¢	VÃ©rifie lâ€™utilisation CPU/MÃ©moire/Disque des brokers.
	â€¢	VÃ©rifie la rÃ©partition des partitions sur les brokers avec :

kafka-replica-verification.sh --broker-list 0,1,2 --topic mon_topic


	â€¢	Assure-toi que la rÃ©plication et les ISR (In-Sync Replicas) sont bien Ã©quilibrÃ©s.

3ï¸âƒ£ AccÃ©lÃ©rer la consommation des messages pour rattraper le LAG

Si le dÃ©bit actuel ne suffit pas Ã  rattraper le retard, voici quelques stratÃ©gies :

âœ… Ajouter plus de consommateurs Logstash (scalabilitÃ© horizontale)
	â€¢	Si un seul Logstash tourne, lance plusieurs instances avec le mÃªme group_id.
	â€¢	ExÃ©cute plusieurs containers ou services Logstash pour parallÃ©liser la consommation.

âœ… Augmenter fetch_max_bytes et max_partition_fetch_bytes
	â€¢	Kafka ne lit que des petits lots de messages par dÃ©faut, ce qui peut limiter la vitesse.
	â€¢	Dans Logstash Kafka Input, augmente la taille du fetch :

input {
  kafka {
    bootstrap_servers => "kafka1:9092,kafka2:9092,kafka3:9092"
    topics => ["mon_topic"]
    group_id => "logstash_group"
    consumer_threads => 4
    max_partition_fetch_bytes => "10485760"  # 10MB
    fetch_max_bytes => "10485760"  # 10MB
  }
}



âœ… Activer auto.commit.interval.ms pour Ã©viter de re-lire des messages
	â€¢	Kafka peut re-lire certains messages si la confirmation de lecture (commit) est trop lente.
	â€¢	Active un intervalle plus rapide dans Logstash :

input {
  kafka {
    bootstrap_servers => "kafka1:9092,kafka2:9092,kafka3:9092"
    topics => ["mon_topic"]
    group_id => "logstash_group"
    consumer_threads => 4
    enable_auto_commit => "true"
    auto_commit_interval_ms => "1000"
  }
}



âœ… Utiliser auto.offset.reset=earliest si des messages sont perdus
	â€¢	Si tu vois des gaps dans les logs et que Kafka ne consomme pas les anciens messages :

input {
  kafka {
    bootstrap_servers => "kafka1:9092,kafka2:9092,kafka3:9092"
    topics => ["mon_topic"]
    group_id => "logstash_group"
    auto_offset_reset => "earliest"
  }
}



âš ï¸ Attention : Cela peut forcer la relecture de tous les messages si le groupe de consommateurs est nouveau.

ðŸ“Œ Conclusion :

âœ… ProblÃ¨me principal :
	â€¢	Le stockage plein a bloquÃ© Kafka, et aprÃ¨s redÃ©marrage, les consumers nâ€™ont pas rattrapÃ© le LAG.
	â€¢	Le LAG continue dâ€™augmenter car les consommateurs ne traitent pas assez rapidement les messages.

âœ… Solutions recommandÃ©es :
	1.	Augmenter les threads Logstash (consumer_threads + pipeline.workers).
	2.	Ajouter plus de partitions pour permettre plus de lecture en parallÃ¨le.
	3.	Augmenter fetch_max_bytes pour lire plus de messages par batch.
	4.	DÃ©ployer plusieurs instances de Logstash pour rÃ©partir la charge.
	5.	Activer auto.commit.interval.ms pour Ã©viter de re-lire trop de messages.
	6.	Surveiller les brokers Kafka et Ã©quilibrer la charge.

ðŸ’¡ Avec ces optimisations, ton Logstash pourra consommer plus rapidement et rÃ©duire le LAG ! ðŸš€


input {
  file {
    path => "/var/log/audit/audit.log"
    type => "audit"
    start_position => "beginning"
  }
}

filter {
  # Parser le type d'Ã©vÃ©nement et le message initial
  grok {
    match => {
      "message" => [
        "^type=%{WORD:audit_type}\s+msg=audit\(%{NUMBER:epoch_time}(?:\.\d+)?(:%{NUMBER:audit_sequence})?\):\s*(?<audit_message>.*)$"
      ]
    }
  }

  # Convertir le temps epoch en @timestamp
  date {
    match => [ "epoch_time", "UNIX" ]
    target => "@timestamp"
    remove_field => [ "epoch_time" ]
  }

  # Parser les paires clÃ©-valeur dans audit_message
  kv {
    source => "audit_message"
    field_split_pattern => "\s+"
    value_split => "="
    include_brackets => false
    trim_value => "\"'"
  }

  # Traiter les valeurs spÃ©cifiques en fonction du type d'audit
  if [audit_type] == "SYSCALL" {
    # Convertir certains champs en types appropriÃ©s
    mutate {
      convert => {
        "pid" => "integer"
        "ppid" => "integer"
        "uid" => "integer"
        "gid" => "integer"
        "euid" => "integer"
        "egid" => "integer"
        "auid" => "integer"
        "ses" => "integer"
        "exit" => "integer"
        "success" => "string"
      }
      lowercase => [ "success" ]
    }
  } else if [audit_type] == "EXECVE" {
    # Combiner les arguments du processus
    mutate {
      rename => { "argc" => "arg_count" }
    }
  } else if [audit_type] == "CWD" {
    # Aucun traitement spÃ©cifique nÃ©cessaire
  } else if [audit_type] == "PATH" {
    # GÃ©rer les chemins de fichiers
    mutate {
      rename => { "name" => "file_path" }
    }
  } else if [audit_type] in ["USER_AUTH", "USER_LOGIN"] {
    # GÃ©rer les Ã©vÃ©nements d'authentification utilisateur
    mutate {
      convert => {
        "pid" => "integer"
        "uid" => "integer"
        "auid" => "integer"
        "ses" => "integer"
      }
    }
  } else if [audit_type] in ["USER_START", "USER_END"] {
    # GÃ©rer le dÃ©but et la fin des sessions utilisateur
    mutate {
      convert => {
        "pid" => "integer"
        "uid" => "integer"
        "auid" => "integer"
        "ses" => "integer"
      }
    }
  } else if [audit_type] in ["CRED_ACQ", "CRED_DISP"] {
    # GÃ©rer l'acquisition et la libÃ©ration des informations d'identification
    mutate {
      convert => {
        "pid" => "integer"
        "uid" => "integer"
        "auid" => "integer"
        "ses" => "integer"
      }
    }
  } else if [audit_type] == "CONFIG_CHANGE" {
    # GÃ©rer les changements de configuration
    mutate {
      convert => {
        "auid" => "integer"
        "pid" => "integer"
        "ses" => "integer"
        "res" => "string"
      }
      lowercase => [ "res" ]
    }
  } else if [audit_type] in ["DAEMON_START", "DAEMON_END"] {
    # GÃ©rer le dÃ©marrage et l'arrÃªt du dÃ©mon d'audit
    mutate {
      convert => {
        "pid" => "integer"
        "uid" => "integer"
        "auid" => "integer"
        "ses" => "integer"
      }
    }
  } else if [audit_type] == "AVC" {
    # GÃ©rer les Ã©vÃ©nements SELinux AVC
    mutate {
      rename => {
        "scontext" => "source_context"
        "tcontext" => "target_context"
        "tclass" => "target_class"
      }
    }
  }

  # Nettoyer les champs inutiles
  mutate {
    remove_field => [ "message", "audit_message" ]
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "linux-audit-%{+YYYY.MM.dd}"
  }
}

filter {
  # Parser le type d'audit et le message initial
  grok {
    match => {
      "message" => "^type=%{WORD:audit_type}\\s+msg=audit\\(%{NUMBER:epoch_time}(?:\\.\\d+)?(?::%{NUMBER:audit_sequence})?\\):\\s*(?<audit_message>.*)$"
    }
  }

  # Convertir le temps epoch en @timestamp
  date {
    match => [ "epoch_time", "UNIX" ]
    target => "@timestamp"
    remove_field => [ "epoch_time" ]
  }

  # Parser les paires clÃ©-valeur dans audit_message
  kv {
    source => "audit_message"
    field_split_pattern => "\\s+"
    value_split => "="
    include_brackets => false
    trim_value => "\"'"
  }

  # Transformations communes pour tous les types d'audit
  mutate {
    # Conversion des champs numÃ©riques
    convert => {
      "pid" => "integer"
      "ppid" => "integer"
      "uid" => "integer"
      "euid" => "integer"
      "gid" => "integer"
      "egid" => "integer"
      "auid" => "integer"
      "ses" => "integer"
      "exit" => "integer"
    }
    # Uniformisation des champs de rÃ©sultat
    lowercase => [ "success", "res" ]
    # Renommage des champs pour une meilleure clartÃ©
    rename => {
      "argc" => "arg_count"
      "name" => "file_path"
    }
    # Suppression des champs temporaires
    remove_field => [ "message", "audit_message" ]
  }

  # SpÃ©cifique pour audit_type = AVC ou USER_AVC
  if [audit_type] in ["AVC", "USER_AVC"] {
    # Extraire le message AVC
    grok {
      match => {
        "msg" => "avc:\\s+%{WORD:avc_action}\\s+\\{\\s*%{WORD:avc_permission}\\s*\\}\\s+for\\s+pid=%{NUMBER:pid}(?:\\s+comm=\"%{DATA:comm}\")?(?:\\s+name=\"%{DATA:file_path}\")?\\s+dev=\"%{DATA:dev}\"\\s+ino=%{NUMBER:inode}\\s+scontext=%{DATA:scontext}\\s+tcontext=%{DATA:tcontext}\\s+tclass=%{DATA:tclass}"
      }
    }
    # Renommer les champs contextuels
    mutate {
      rename => {
        "scontext" => "source_context"
        "tcontext" => "target_context"
        "tclass" => "target_class"
      }
    }
  }
}
filter {
  # Parser le type d'audit et le message initial
  grok {
    match => {
      "message" => "^type=%{WORD:audit_type}\\s+msg=audit\\(%{NUMBER:epoch_time}(?:\\.\\d+)?(?::%{NUMBER:audit_sequence})?\\):\\s*(?<audit_message>.*)$"
    }
  }

  # Convertir le temps epoch en @timestamp
  date {
    match => [ "epoch_time", "UNIX" ]
    target => "@timestamp"
    remove_field => [ "epoch_time" ]
  }

  # Parser les paires clÃ©-valeur dans audit_message
  kv {
    source => "audit_message"
    field_split_pattern => "\\s+"
    value_split => "="
    include_brackets => false
    trim_value => "\"'"
  }

  # Transformations communes pour tous les types d'audit
  mutate {
    # Conversion des champs numÃ©riques
    convert => {
      "pid" => "integer"
      "ppid" => "integer"
      "uid" => "integer"
      "euid" => "integer"
      "gid" => "integer"
      "egid" => "integer"
      "auid" => "integer"
      "ses" => "integer"
      "exit" => "integer"
    }
    # Uniformisation des champs de rÃ©sultat
    lowercase => [ "success", "res" ]
    # Renommage des champs pour une meilleure clartÃ©
    rename => {
      "argc" => "arg_count"
      "name" => "file_path"
    }
    # Suppression des champs temporaires
    remove_field => [ "message", "audit_message" ]
  }

  # SpÃ©cifique pour audit_type = AVC ou USER_AVC
  if [audit_type] in ["AVC", "USER_AVC"] {
    # Utiliser grok pour parser jusqu'Ã  "} for"
    grok {
      match => {
        "msg" => "avc:\\s+%{WORD:avc_action}\\s+\\{\\s*%{WORD:avc_permission}\\s*\\}\\s+for\\s*(?<remaining_msg>.*)"
      }
    }

    # Utiliser kv pour le reste du message
    kv {
      source => "remaining_msg"
      field_split_pattern => "\\s+"
      value_split => "="
      trim_value => "\"'"
    }

    # Renommer les champs contextuels
    mutate {
      rename => {
        "scontext" => "source_context"
        "tcontext" => "target_context"
        "tclass" => "target_class"
      }
      remove_field => [ "remaining_msg" ]
    }
  }
}

filter {
  # Ã‰tape 1 : Arrondir le timestamp Ã  la seconde pour ignorer les millisecondes
  ruby {
    code => "
      event.set('rounded_timestamp', event.get('@timestamp').time.strftime('%Y-%m-%d %H:%M:%S'))
    "
  }

  # Ã‰tape 2 : GÃ©nÃ©rer un hash unique basÃ© sur les champs clÃ©s (excluant les millisecondes)
  fingerprint {
    source => ["rounded_timestamp", "program", "message"]
    target => "message_hash"
    method => "SHA1"
  }

  # Ã‰tape 3 : Utiliser l'aggregate filter pour dÃ©tecter et ignorer les doublons
  aggregate {
    task_id => "%{message_hash}"
    code => "
      map['count'] ||= 0;
      map['count'] += 1;
      if (map['count'] > 1)
        event.cancel()
    "
    timeout => 60 # Cache chaque message unique pendant 60 secondes
  }

  # Optionnel : Ajouter un tag si un doublon est dÃ©tectÃ© pour une analyse ultÃ©rieure
  if [message_hash] and [count] > 1 {
    mutate {
      add_tag => ["_duplicate"]
    }
  }

  # Supprimer le champ temporaire 'rounded_timestamp'
  mutate {
    remove_field => ["rounded_timestamp"]
  }
}
